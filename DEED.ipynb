{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb87d82c",
   "metadata": {},
   "source": [
    "# Dynamic Economic Emissions Dispatch (DEED) via Reinforcement Learning\n",
    "\n",
    "This notebook demonstrates how to solve the **Dynamic Economic Emissions Dispatch** (DEED) problem using modern reinforcement learning (PPO from Stable Baselines3).\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "The DEED problem is a multi-objective optimization problem in power systems engineering. Given a set of $N$ thermal generating units and a 24-hour demand profile, the goal is to determine the power output of each generator at each hour to **minimize both fuel cost and pollutant emissions** while satisfying:\n",
    "\n",
    "1. **Power balance constraint**: Total generation must meet demand plus transmission losses\n",
    "2. **Generation limits**: $P_i^{\\min} \\leq P_i^m \\leq P_i^{\\max}$\n",
    "3. **Ramp rate limits**: $-DR_i \\leq P_i^m - P_i^{m-1} \\leq UR_i$\n",
    "\n",
    "### Cost Function\n",
    "\n",
    "$$F_C = \\sum_{m=1}^{M} \\sum_{i=1}^{N} \\left[ a_i + b_i P_i^m + c_i (P_i^m)^2 + |d_i \\sin(e_i (P_i^{\\min} - P_i^m))| \\right]$$\n",
    "\n",
    "The sinusoidal term models the **valve-point loading effect** in steam turbines.\n",
    "\n",
    "### Emissions Function\n",
    "\n",
    "$$F_E = \\sum_{m=1}^{M} \\sum_{i=1}^{N} E \\left[ \\alpha_i + \\beta_i P_i^m + \\gamma_i (P_i^m)^2 + \\eta_i \\exp(\\delta_i P_i^m) \\right]$$\n",
    "\n",
    "### Combined Objective (CEED)\n",
    "\n",
    "Using linear scalarization with weights $W_C$ and $W_E$:\n",
    "\n",
    "$$\\min \\; W_C \\cdot F_C + W_E \\cdot F_E$$\n",
    "\n",
    "subject to power balance, generation limits, and ramp rate constraints.\n",
    "\n",
    "## Test System\n",
    "\n",
    "We use a standard **IEEE 10-generator benchmark** system with:\n",
    "- 10 thermal generating units with known cost/emission coefficients\n",
    "- Kron's B-matrix for transmission loss calculation\n",
    "- 24-hour load demand profile (peak: 2150 MW at hour 12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192bd9a7",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Environment Setup\n",
    "\n",
    "We use a custom Gymnasium environment (`deed_env.py`) that models the DEED problem. The agent controls 9 generators (units 2-10) sequentially; unit 1 is the \"slack\" generator determined by the power balance equation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the DEED environment\n",
    "from deed_env import DEEDEnv\n",
    "\n",
    "print(\"Environment module loaded successfully.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b14b1b",
   "metadata": {},
   "source": [
    "### Create and Explore the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2519b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = DEEDEnv(Wc=0.5, We=0.5)\n",
    "obs, info = env.reset(seed=42)\n",
    "\n",
    "print(\"=== DEED Environment ===\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\")\n",
    "print(f\"Number of generators: {env.N}\")\n",
    "print(f\"Scheduling horizon: {env.M} hours\")\n",
    "print(f\"Steps per episode: {env.M * (env.N - 1)} (24 hours x 9 generators)\")\n",
    "print()\n",
    "print(f\"Initial observation (14-dim vector):\")\n",
    "print(f\"  {obs}\")\n",
    "print()\n",
    "print(f\"Initial info: {info}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27970168",
   "metadata": {},
   "source": [
    "### Generator Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6983d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display generator data\n",
    "gen_cols = [\"P_min\", \"P_max\", \"a\", \"b\", \"c\", \"d\", \"e\",\n",
    "            \"alpha\", \"beta\", \"gamma\", \"eta\", \"delta\", \"UR\", \"DR\"]\n",
    "gen_df = pd.DataFrame(env.GEN_DATA, columns=gen_cols,\n",
    "                      index=[f\"Unit {i+1}\" for i in range(10)])\n",
    "print(\"Generator Characteristics (IEEE 10-Unit Benchmark):\")\n",
    "display(gen_df[[\"P_min\", \"P_max\", \"UR\", \"DR\"]].astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28614999",
   "metadata": {},
   "source": [
    "### 24-Hour Demand Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ddaa83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the demand profile\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "hours = np.arange(1, 25)\n",
    "ax.bar(hours, env.DEMAND, color='steelblue', alpha=0.8, edgecolor='navy')\n",
    "ax.set_xlabel('Hour', fontsize=12)\n",
    "ax.set_ylabel('Power Demand (MW)', fontsize=12)\n",
    "ax.set_title('24-Hour Power Demand Profile', fontsize=14)\n",
    "ax.set_xticks(hours)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('demand_profile.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(f\"Peak demand: {env.DEMAND.max():.0f} MW at hour {np.argmax(env.DEMAND)+1}\")\n",
    "print(f\"Minimum demand: {env.DEMAND.min():.0f} MW at hour {np.argmin(env.DEMAND)+1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d31588",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 3: Training with PPO (Proximal Policy Optimization)\n",
    "\n",
    "We use **Stable Baselines3's PPO** algorithm to train an RL agent for the DEED problem. PPO is a modern policy gradient method that is well-suited for environments with continuous or large discrete action spaces.\n",
    "\n",
    "This replaces the original tabular Q-learning approach, which was fundamentally inadequate for this problem due to the enormous state-action space (216 steps x 101 actions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248ed1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "class RewardLoggerCallback(BaseCallback):\n",
    "    \"\"\"Custom callback to log episode rewards during training.\"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_costs = []\n",
    "        self.episode_emissions = []\n",
    "        self._current_reward = 0.0\n",
    "\n",
    "    def _on_step(self):\n",
    "        # Check if episode ended\n",
    "        infos = self.locals.get(\"infos\", [{}])\n",
    "        dones = self.locals.get(\"dones\", [False])\n",
    "        for i, done in enumerate(dones):\n",
    "            if done and \"total_cost\" in infos[i]:\n",
    "                self.episode_rewards.append(infos[i].get(\"total_cost\", 0) + infos[i].get(\"total_emissions\", 0))\n",
    "                self.episode_costs.append(infos[i].get(\"total_cost\", 0))\n",
    "                self.episode_emissions.append(infos[i].get(\"total_emissions\", 0))\n",
    "        return True\n",
    "\n",
    "print(\"Stable Baselines3 imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9415cded",
   "metadata": {},
   "source": [
    "### Train the PPO Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0400b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "TOTAL_TIMESTEPS = 50_000  # Increase for better results (e.g., 200_000)\n",
    "SEED = 42\n",
    "\n",
    "# Create environment\n",
    "train_env = DEEDEnv(Wc=0.5, We=0.5)\n",
    "\n",
    "# Create PPO agent\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    train_env,\n",
    "    learning_rate=3e-4,\n",
    "    n_steps=216 * 4,      # Collect 4 full episodes before update\n",
    "    batch_size=216,        # One full episode per batch\n",
    "    n_epochs=10,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    verbose=1,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Train with callback\n",
    "callback = RewardLoggerCallback()\n",
    "print(f\"Training PPO for {TOTAL_TIMESTEPS:,} timesteps...\")\n",
    "print(f\"This is approximately {TOTAL_TIMESTEPS // 216} episodes.\")\n",
    "print()\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callback, progress_bar=False)\n",
    "print()\n",
    "print(f\"Training complete. Episodes logged: {len(callback.episode_costs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483bee7f",
   "metadata": {},
   "source": [
    "### Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f8e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "if len(callback.episode_costs) > 0:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Smoothing function\n",
    "    def smooth(y, window=10):\n",
    "        if len(y) < window:\n",
    "            return y\n",
    "        return pd.Series(y).rolling(window=window, min_periods=1).mean().values\n",
    "\n",
    "    episodes = np.arange(1, len(callback.episode_costs) + 1)\n",
    "\n",
    "    # Cost curve\n",
    "    axes[0].plot(episodes, callback.episode_costs, alpha=0.3, color='blue', label='Raw')\n",
    "    axes[0].plot(episodes, smooth(callback.episode_costs), color='blue', linewidth=2, label='Smoothed')\n",
    "    axes[0].set_xlabel('Episode', fontsize=12)\n",
    "    axes[0].set_ylabel('Total Fuel Cost ($/day)', fontsize=12)\n",
    "    axes[0].set_title('Training: Total Fuel Cost', fontsize=14)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "    # Emissions curve\n",
    "    axes[1].plot(episodes, callback.episode_emissions, alpha=0.3, color='green', label='Raw')\n",
    "    axes[1].plot(episodes, smooth(callback.episode_emissions), color='green', linewidth=2, label='Smoothed')\n",
    "    axes[1].set_xlabel('Episode', fontsize=12)\n",
    "    axes[1].set_ylabel('Total Emissions (tons/day)', fontsize=12)\n",
    "    axes[1].set_title('Training: Total Emissions', fontsize=14)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No episode data logged during training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d560b57",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 4: Results Analysis\n",
    "\n",
    "Evaluate the trained PPO agent and visualize the dispatch schedule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef9641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained agent\n",
    "eval_env = DEEDEnv(Wc=0.5, We=0.5)\n",
    "obs, info = eval_env.reset(seed=99)\n",
    "\n",
    "total_reward = 0.0\n",
    "terminated = False\n",
    "while not terminated:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = eval_env.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(\"=== PPO Agent Evaluation ===\")\n",
    "print(f\"Total reward: {total_reward:,.2f}\")\n",
    "print(f\"Total fuel cost: {info['total_cost']:,.2f} $/day\")\n",
    "print(f\"Total emissions: {info['total_emissions']:,.2f} tons/day\")\n",
    "print()\n",
    "\n",
    "# Get dispatch schedule\n",
    "schedule_df = eval_env.get_dispatch_schedule()\n",
    "print(\"Dispatch Schedule (MW):\")\n",
    "display(schedule_df.round(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d929e3fc",
   "metadata": {},
   "source": [
    "### Dispatch Schedule Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c730ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot dispatch schedule as stacked area chart\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "hours = np.arange(1, 25)\n",
    "schedule = eval_env.p_schedule  # (10, 24)\n",
    "\n",
    "# Stacked area chart\n",
    "bottom = np.zeros(24)\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, 10))\n",
    "for i in range(10):\n",
    "    power = np.maximum(schedule[i], 0)  # Avoid negative in plot\n",
    "    ax.bar(hours, power, bottom=bottom, color=colors[i], \n",
    "           label=f'Unit {i+1}', alpha=0.85, width=0.8)\n",
    "    bottom += power\n",
    "\n",
    "ax.plot(hours, eval_env.DEMAND, 'k--', linewidth=2, label='Demand', marker='o', markersize=4)\n",
    "ax.set_xlabel('Hour', fontsize=12)\n",
    "ax.set_ylabel('Power (MW)', fontsize=12)\n",
    "ax.set_title('Optimal Dispatch Schedule (PPO Agent)', fontsize=14)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=9)\n",
    "ax.set_xticks(hours)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('dispatch_schedule.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd6187",
   "metadata": {},
   "source": [
    "### Hourly Cost and Emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a92d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hourly cost and emissions\n",
    "hourly_costs = eval_env.get_hourly_costs()\n",
    "hourly_emissions = eval_env.get_hourly_emissions()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "hours = np.arange(1, 25)\n",
    "\n",
    "axes[0].bar(hours, hourly_costs, color='coral', alpha=0.8, edgecolor='darkred')\n",
    "axes[0].set_xlabel('Hour', fontsize=12)\n",
    "axes[0].set_ylabel('Fuel Cost ($)', fontsize=12)\n",
    "axes[0].set_title('Hourly Fuel Cost', fontsize=14)\n",
    "axes[0].set_xticks(hours)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "axes[1].bar(hours, hourly_emissions, color='seagreen', alpha=0.8, edgecolor='darkgreen')\n",
    "axes[1].set_xlabel('Hour', fontsize=12)\n",
    "axes[1].set_ylabel('Emissions (tons)', fontsize=12)\n",
    "axes[1].set_title('Hourly Emissions', fontsize=14)\n",
    "axes[1].set_xticks(hours)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hourly_cost_emissions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total daily cost: {hourly_costs.sum():,.2f} $\")\n",
    "print(f\"Total daily emissions: {hourly_emissions.sum():,.2f} tons\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62612d",
   "metadata": {},
   "source": [
    "### Comparison with Equal Dispatch Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322a4ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equal dispatch baseline: each generator outputs equal share of demand\n",
    "baseline_env = DEEDEnv(Wc=0.5, We=0.5)\n",
    "baseline_env.reset(seed=99)\n",
    "\n",
    "for m in range(24):\n",
    "    demand = baseline_env.DEMAND[m]\n",
    "    total_capacity = sum(baseline_env.GEN_DATA[n, baseline_env.P_MAX] for n in range(10))\n",
    "    for n in range(10):\n",
    "        p_min = baseline_env.GEN_DATA[n, baseline_env.P_MIN]\n",
    "        p_max = baseline_env.GEN_DATA[n, baseline_env.P_MAX]\n",
    "        # Proportional dispatch based on capacity\n",
    "        share = (p_max / total_capacity) * demand\n",
    "        power = np.clip(share, p_min, p_max)\n",
    "        baseline_env.p_schedule[n, m] = power\n",
    "\n",
    "baseline_cost = sum(baseline_env._fuel_cost_total(m) for m in range(24))\n",
    "baseline_emissions = sum(baseline_env._emissions_total(m) for m in range(24))\n",
    "\n",
    "print(\"=== Comparison ===\")\n",
    "print(f\"{'Method':<25} {'Cost ($)':>15} {'Emissions (tons)':>20}\")\n",
    "print(\"-\" * 62)\n",
    "print(f\"{'PPO Agent':<25} {info['total_cost']:>15,.2f} {info['total_emissions']:>20,.2f}\")\n",
    "print(f\"{'Proportional Dispatch':<25} {baseline_cost:>15,.2f} {baseline_emissions:>20,.2f}\")\n",
    "if info['total_cost'] < baseline_cost:\n",
    "    pct = (1 - info['total_cost']/baseline_cost) * 100\n",
    "    print(f\"\\nPPO achieves {pct:.1f}% cost reduction over proportional dispatch.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d6d0bc",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 5: Multi-Objective Pareto Front Analysis\n",
    "\n",
    "This section presents **new research**: by training multiple PPO agents with different weight combinations $(W_C, W_E)$, we can trace out the **Pareto front** of the Combined Economic and Emissions Dispatch (CEED) problem.\n",
    "\n",
    "The Pareto front shows the trade-off between fuel cost and emissions -- solutions where improving one objective necessarily worsens the other. This is a key contribution: using RL to approximate the Pareto front without requiring traditional mathematical programming.\n",
    "\n",
    "### Methodology\n",
    "1. Choose a set of weight pairs: $(W_C, W_E) \\in \\{(1.0, 0.0), (0.8, 0.2), ..., (0.0, 1.0)\\}$\n",
    "2. Train a PPO agent for each weight pair\n",
    "3. Evaluate each agent and record total cost and emissions\n",
    "4. Plot the Pareto front\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ada386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pareto front analysis\n",
    "# Define weight combinations (Wc + We = 1.0)\n",
    "weight_pairs = [\n",
    "    (1.0, 0.0),   # Pure cost minimization\n",
    "    (0.9, 0.1),\n",
    "    (0.8, 0.2),\n",
    "    (0.7, 0.3),\n",
    "    (0.6, 0.4),\n",
    "    (0.5, 0.5),   # Equal weighting\n",
    "    (0.4, 0.6),\n",
    "    (0.3, 0.7),\n",
    "    (0.2, 0.8),\n",
    "    (0.1, 0.9),\n",
    "    (0.0, 1.0),   # Pure emissions minimization\n",
    "]\n",
    "\n",
    "PARETO_TIMESTEPS = 30_000  # Timesteps per weight pair (increase for better results)\n",
    "\n",
    "pareto_results = []\n",
    "\n",
    "print(f\"Training {len(weight_pairs)} agents for Pareto front analysis...\")\n",
    "print(f\"Timesteps per agent: {PARETO_TIMESTEPS:,}\")\n",
    "print()\n",
    "\n",
    "for idx, (wc, we) in enumerate(weight_pairs):\n",
    "    print(f\"[{idx+1}/{len(weight_pairs)}] Wc={wc:.1f}, We={we:.1f} ... \", end=\"\", flush=True)\n",
    "    \n",
    "    # Create environment with these weights\n",
    "    pareto_env = DEEDEnv(Wc=wc, We=we)\n",
    "    \n",
    "    # Train PPO agent\n",
    "    pareto_model = PPO(\n",
    "        \"MlpPolicy\",\n",
    "        pareto_env,\n",
    "        learning_rate=3e-4,\n",
    "        n_steps=216 * 4,\n",
    "        batch_size=216,\n",
    "        n_epochs=10,\n",
    "        gamma=0.99,\n",
    "        ent_coef=0.01,\n",
    "        verbose=0,\n",
    "        seed=42,\n",
    "    )\n",
    "    pareto_model.learn(total_timesteps=PARETO_TIMESTEPS, progress_bar=False)\n",
    "    \n",
    "    # Evaluate (best of 3 runs)\n",
    "    best_cost = float('inf')\n",
    "    best_emissions = float('inf')\n",
    "    best_combined = float('inf')\n",
    "    \n",
    "    for eval_seed in [99, 100, 101]:\n",
    "        eval_env = DEEDEnv(Wc=wc, We=we)\n",
    "        obs, _ = eval_env.reset(seed=eval_seed)\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            action, _ = pareto_model.predict(obs, deterministic=True)\n",
    "            obs, _, terminated, _, eval_info = eval_env.step(action)\n",
    "        \n",
    "        cost = eval_info['total_cost']\n",
    "        emissions = eval_info['total_emissions']\n",
    "        combined = wc * cost + we * emissions\n",
    "        \n",
    "        if combined < best_combined:\n",
    "            best_cost = cost\n",
    "            best_emissions = emissions\n",
    "            best_combined = combined\n",
    "    \n",
    "    pareto_results.append({\n",
    "        'Wc': wc,\n",
    "        'We': we,\n",
    "        'cost': best_cost,\n",
    "        'emissions': best_emissions,\n",
    "    })\n",
    "    print(f\"Cost={best_cost:,.0f}, Emissions={best_emissions:,.0f}\")\n",
    "\n",
    "print(\"\\nPareto front analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b88f6c",
   "metadata": {},
   "source": [
    "### Pareto Front Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a27abe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Pareto front\n",
    "costs = [r['cost'] for r in pareto_results]\n",
    "emissions = [r['emissions'] for r in pareto_results]\n",
    "wcs = [r['Wc'] for r in pareto_results]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "# Plot Pareto front\n",
    "scatter = ax.scatter(costs, emissions, c=wcs, cmap='RdYlGn', s=120,\n",
    "                     edgecolors='black', linewidth=1.5, zorder=5)\n",
    "ax.plot(costs, emissions, 'k--', alpha=0.5, linewidth=1, zorder=3)\n",
    "\n",
    "# Annotate points with weight values\n",
    "for i, r in enumerate(pareto_results):\n",
    "    offset_x = (max(costs) - min(costs)) * 0.02\n",
    "    offset_y = (max(emissions) - min(emissions)) * 0.02\n",
    "    ax.annotate(f\"({r['Wc']:.1f}, {r['We']:.1f})\",\n",
    "                (r['cost'], r['emissions']),\n",
    "                textcoords=\"offset points\", xytext=(8, 8),\n",
    "                fontsize=8, alpha=0.8)\n",
    "\n",
    "cbar = plt.colorbar(scatter, ax=ax, label='Cost Weight (Wc)')\n",
    "ax.set_xlabel('Total Fuel Cost ($/day)', fontsize=13)\n",
    "ax.set_ylabel('Total Emissions (tons/day)', fontsize=13)\n",
    "ax.set_title('Pareto Front: Cost vs Emissions Trade-off (DEED via RL)', fontsize=14)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('pareto_front.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print results table\n",
    "print(\"\\n=== Pareto Front Results ===\")\n",
    "print(f\"{'Wc':>5} {'We':>5} {'Cost ($)':>15} {'Emissions (tons)':>20}\")\n",
    "print(\"-\" * 50)\n",
    "for r in pareto_results:\n",
    "    print(f\"{r['Wc']:>5.1f} {r['We']:>5.1f} {r['cost']:>15,.2f} {r['emissions']:>20,.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8098be6",
   "metadata": {},
   "source": [
    "### Analysis of the Pareto Front\n",
    "\n",
    "The Pareto front reveals several key insights:\n",
    "\n",
    "1. **Trade-off structure**: There is a clear inverse relationship between cost and emissions. Cheaper dispatch schedules produce more emissions, and vice versa.\n",
    "\n",
    "2. **Diminishing returns**: The marginal cost of emissions reduction increases as we move toward the emissions-minimized end of the front.\n",
    "\n",
    "3. **Decision support**: The Pareto front provides decision-makers with a menu of optimal operating points. The \"knee\" of the front (around $W_C = 0.5, W_E = 0.5$) often represents a good compromise.\n",
    "\n",
    "4. **RL advantage**: Unlike traditional methods (weighted sum LP/QP, epsilon-constraint), the RL approach naturally handles the non-convex valve-point loading effects and can be extended to more complex constraints.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "- Increase training timesteps for more converged Pareto points\n",
    "- Use multi-objective RL algorithms (e.g., MORL) for direct Pareto optimization\n",
    "- Add renewable energy sources and battery storage to the environment\n",
    "- Compare RL results with classical optimization benchmarks (PSO, DE, NSGA-II)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f187fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=== DEED Notebook Summary ===\")\n",
    "print(f\"Environment: {env.N} generators, {env.M} hours\")\n",
    "print(f\"Steps per episode: {env.M * (env.N - 1)}\")\n",
    "print(f\"Pareto front points: {len(pareto_results)}\")\n",
    "print()\n",
    "print(\"Files created:\")\n",
    "print(\"  - deed_env.py: Gymnasium DEED environment\")\n",
    "print(\"  - tests/test_deed_env.py: 37 unit tests\")\n",
    "print(\"  - requirements.txt: Python dependencies\")\n",
    "print(\"  - DEED.ipynb: This notebook\")\n",
    "print()\n",
    "print(\"To run tests: pytest tests/ -v\")\n",
    "print(\"To run notebook: jupyter notebook DEED.ipynb\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
